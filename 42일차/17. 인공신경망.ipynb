{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "a9RDaxFMMBWR",
        "outputId": "890cfd4d-5972-4abd-864e-bc7ef5f1c1e4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3d009bd72038>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    실제 인간의 뇌에서 수행되는 신경망 방식을 사용한 컴퓨터 연산 방법\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# 인공신경망\n",
        "실제 인간의 뇌에서 수행되는 신경망 방식을 사용한 컴퓨터 연산 방법   \n",
        "실제 뇌에서 뉴런간 정보 전달 방식을 모델링한 것"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 퍼셉트론\n",
        "신경망의 한 종류   \n",
        "\n",
        "입력값 X 를 가중치와 곱해서 모두 더한 값이 임계값 기준이 있는 노드"
      ],
      "metadata": {
        "id": "NWJiiIprMDww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "rhHhGPXAMDQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    self.x1 = df.iloc[:, 0].values\n",
        "    self.x2 = df.iloc[:,1].values\n",
        "    self.y = df.iloc[:,2].values\n",
        "    self.length = len(df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor([self.x1[index], self.x2[index]])\n",
        "    y = torch.FloatTensor([self.y[index]])\n",
        "    return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomModel, self).__init__()\n",
        "\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Linear(2,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer(x)\n",
        "    return x\n",
        "\n",
        "train_dataset = CustomDataset('data.csv')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "model = CustomModel().to(device)\n",
        "criterion = nn.BCELoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(10000):\n",
        "  cost = 0.0\n",
        "  for x,y in train_dataloader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    cost += loss\n",
        "\n",
        "  cost = cost/len(train_dataloader)\n",
        "\n",
        "  if (epoch + 1) % 1000 ==0:\n",
        "    print(f'Epoch : {epoch+1:4d}, Cost : {cost:.3f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  input = torch.FloatTensor([\n",
        "      [0,0],\n",
        "      [0,1],\n",
        "      [1,0],\n",
        "      [1,1]\n",
        "  ]).to(device)\n",
        "\n",
        "  outputs = model(input)\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "HzRHF4liMFst",
        "outputId": "25374b1b-7a74-4197-e921-8d5bad857a37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-857260b7eeeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-857260b7eeeb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class fruit():\n",
        "  def __init__(self) -> None:\n",
        "    self._cnt = 0\n",
        "    self._name = []\n",
        "\n",
        "  def add_fruit(self, name, size):\n",
        "     self._name.append(name)\n",
        "     self._cnt += 1\n",
        "     return True\n",
        "\n",
        "  def set_fruit(self, new_name):\n",
        "    self._name = new_name\n",
        "\n",
        "  def get_item(self):\n",
        "    return self._name\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._cnt"
      ],
      "metadata": {
        "id": "FAAgYoL8MHn6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_class = fruit()"
      ],
      "metadata": {
        "id": "t9FtJTZ4MJe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_class.add_fruit('apple',10)\n",
        "fruit_class.add_fruit('banana',10)\n",
        "fruit_class.add_fruit('orange',10)"
      ],
      "metadata": {
        "id": "Pac7zWWeMLFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(fruit_class)"
      ],
      "metadata": {
        "id": "M9w_jhIMMLtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(fruit_class)"
      ],
      "metadata": {
        "id": "2p60-iTbMMgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_class.get_item()"
      ],
      "metadata": {
        "id": "yBvhcnbRMNV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_class.name = ['1','2','3']\n",
        "# 직접 접근해서 바꿀 수도 있다다"
      ],
      "metadata": {
        "id": "g76fCoKqMN8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_class.name"
      ],
      "metadata": {
        "id": "Jp6RG0ONMOl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다층퍼셉트론 모델"
      ],
      "metadata": {
        "id": "Do4_vjsiMSew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    self.x1 = df.iloc[:, 0].values\n",
        "    self.x2 = df.iloc[:,1].values\n",
        "    self.y = df.iloc[:,2].values\n",
        "    self.length = len(df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor([self.x1[index], self.x2[index]])\n",
        "    y = torch.FloatTensor([self.y[index]])\n",
        "    return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomModel, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Linear(2,2),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Linear(2,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    return x\n",
        "\n",
        "train_dataset = CustomDataset('data.csv')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "model = CustomModel().to(device)\n",
        "criterion = nn.BCELoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(10000):\n",
        "  cost = 0.0\n",
        "  for x,y in train_dataloader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    cost += loss\n",
        "\n",
        "  cost = cost/len(train_dataloader)\n",
        "\n",
        "  if (epoch + 1) % 1000 ==0:\n",
        "    print(f'Epoch : {epoch+1:4d}, Cost : {cost:.3f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  input = torch.FloatTensor([\n",
        "      [0,0],\n",
        "      [0,1],\n",
        "      [1,0],\n",
        "      [1,1]\n",
        "  ]).to(device)\n",
        "\n",
        "  outputs = model(input)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs<=0.5)"
      ],
      "metadata": {
        "id": "dc-qqSiyMTKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install koreanize-matplotlib\n",
        "import numpy as np\n",
        "import koreanize_matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "x68w6DcYMU_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(x):\n",
        "  return np.array(x >0, dtype=np.int)\n",
        "\n",
        "x = np.arange(-5.0,5.0,0.1)\n",
        "y = step(x)\n",
        "plt.title('Step Function')\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7sJfqYxOMV0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시크모이드 함수\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "y = sigmoid(x)\n",
        "plt.plot(x,y)\n",
        "plt.title('Sigmoid Function')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JB1MB0u5MWl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(-5.0,5.0,0.1)\n",
        "y = np.tanh(x)\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.title('Tanh function')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DeG3bhOFMXa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "plt.plot(x,relu(x))\n",
        "plt.grid()\n",
        "plt.title('ReLU Function')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2ytt04GMYPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = 0.1\n",
        "\n",
        "def leaky_relu(x):\n",
        "  return np.maximum(a*x,x)\n",
        "\n",
        "plt.plot(x,leaky_relu(x))\n",
        "plt.grid()\n",
        "plt.title('Leaky ReLU')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VP46miH9MZEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax\n",
        "x = np.arange(-5.0,5.0,0.1)\n",
        "y = np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.title('Softmax Function')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6CVV_2nsMZ5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbors\n",
        "\n",
        "새로운 데이터(x)와 가장 가까운 k개의 데이터를 통해 x를 분류하는 방법   \n",
        "\n",
        "- euclidean distance\n",
        "$$\\sqrt{(x_1-μ_1)^2 + (x_2-μ_2)^2+ ⋯ + (x_p-μ_p)^2}$$\n",
        "\n",
        "- manhattan distance\n",
        "$$|x_1-μ_1|+|x_2-μ_2|+⋯+|x_p-μ_p|$$\n",
        "\n",
        "      변수의 값이 가지는 스케일의 차이가 모델 학습에 영향을 미치는 것을 막기 위해 스케일링 수행이 필요\n",
        "      각 변수의 값 차이가 가졌던 정보는 남아있돌록 스케일링"
      ],
      "metadata": {
        "id": "vqaxntn0MbiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn as sc"
      ],
      "metadata": {
        "id": "lFvUiKERMc-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_price = pd.read_csv('train.csv')\n",
        "print(mobile_price.shape)\n",
        "mobile_price.head()"
      ],
      "metadata": {
        "id": "jWmfm8vtMeIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_price['price_range'].unique()"
      ],
      "metadata": {
        "id": "PNdpLjozMe9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_price.columns"
      ],
      "metadata": {
        "id": "bXTBvhylMfW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = mobile_price.columns\n",
        "\n",
        "X = mobile_price[columns[:-1]]\n",
        "y = mobile_price[columns[-1]]\n",
        "\n",
        "SC = StandardScaler()\n",
        "X = SC.fit_transform(X)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "3uKHxMUvMgK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train),len(x_test))"
      ],
      "metadata": {
        "id": "1W92-rzwMhz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 12):\n",
        "  knn_model=KNeighborsClassifier(n_neighbors=i,\n",
        "                                metric='manhattan').fit(x_train, y_train)\n",
        "  print(knn_model.score(x_train, y_train))\n",
        "  print(knn_model.score(x_test, y_test))\n",
        "  print('='*10)"
      ],
      "metadata": {
        "id": "H-KyumVCMjM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model=KNeighborsClassifier(n_neighbors=8,\n",
        "                              metric='manhattan').fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "DL7vZT9XMkP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model.predict_proba(x_test)[0]"
      ],
      "metadata": {
        "id": "LsP4oYF4Mk57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model.predict(x_test)[0]"
      ],
      "metadata": {
        "id": "qUmVPoFEMlpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glass = pd.read_csv('glass.csv')\n",
        "glass"
      ],
      "metadata": {
        "id": "AFH_MqprMmaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glass.Type.unique()"
      ],
      "metadata": {
        "id": "iaJVdY-EMnP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    self.x1 = df.iloc[:, 0].values\n",
        "    self.x2 = df.iloc[:,1].values\n",
        "    self.x3 = df.iloc[:, 2].values\n",
        "    self.x4 = df.iloc[:,3].values\n",
        "    self.x5 = df.iloc[:, 4].values\n",
        "    self.x6 = df.iloc[:,5].values\n",
        "    self.x7 = df.iloc[:, 6].values\n",
        "    self.x8 = df.iloc[:,7].values\n",
        "    self.x9 = df.iloc[:,8].values\n",
        "    self.y = df.iloc[:,9].values\n",
        "    self.length = len(df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor([self.x1[index], self.x2[index],self.x3[index], \n",
        "                           self.x4[index],self.x5[index], self.x6[index],\n",
        "                           self.x7[index], self.x8[index],self.x9[index]])\n",
        "    y = torch.LongTensor(self.y-1)[index]\n",
        "    return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomModel, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Linear(9,256),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Linear(256,512),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Linear(512,128),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(128,18),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.output_layer = nn.Sequential(\n",
        "        nn.Linear(18,7),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    return x\n",
        "\n",
        "train_dataset = CustomDataset('glass.csv')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "\n",
        "device = 'cuda'\n",
        "print(device)\n",
        "\n",
        "model = CustomModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100000):\n",
        "  cost = 0.0\n",
        "  for x,y in train_dataloader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    cost += loss\n",
        "\n",
        "  cost = cost/len(train_dataloader)\n",
        "\n",
        "  if (epoch + 1) % 1000 ==0:\n",
        "    print(f'Epoch : {epoch+1:4d}, Cost : {cost:.3f}')\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   model.eval()\n",
        "#   print(glass[:10,:-1])\n",
        "#   input = torch.FloatTensor([\n",
        "#       [0,0],\n",
        "#       [0,1],\n",
        "#       [1,0],\n",
        "#       [1,1]\n",
        "#   ]).to(device)\n",
        "\n",
        "#   outputs = model(input)\n",
        "\n",
        "# print(outputs)\n",
        "# print(outputs<=0.5)"
      ],
      "metadata": {
        "id": "PRuhZX2IMoGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('dataset.csv')['class'].unique()"
      ],
      "metadata": {
        "id": "sbtGpT2NMprp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    self.a = df.iloc[:, 0].values\n",
        "    self.b = df.iloc[:,1].values\n",
        "    self.c = df.iloc[:, 2].values\n",
        "    self.y = df.iloc[:, 3].values\n",
        "    self.y = list(map(self.string_to_vector, self.y))\n",
        "    self.length = len(df)\n",
        "\n",
        "  def string_to_vector(self, value):\n",
        "    key_value = {'obtuse triangle':2, 'acute triangle':1, 'right triangle':0}\n",
        "    return key_value[value]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(sorted([self.a[index], self.b[index], self.c[index]]))\n",
        "    y = torch.LongTensor(self.y)[index]\n",
        "    return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomModel, self).__init__()\n",
        "\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Linear(3, 3)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layer(x)\n",
        "\n",
        "train_dataset = CustomDataset('dataset.csv')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "model = CustomModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10000):\n",
        "  cost = 0.0\n",
        "  for x, y in train_dataloader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    cost += loss\n",
        "\n",
        "  cost = cost/len(train_dataloader)\n",
        "\n",
        "  if (epoch + 1) % 1000 ==0:\n",
        "    print(f'Epoch : {epoch+1:4d}, Cost : {cost:.3f}')\n"
      ],
      "metadata": {
        "id": "liF2vpAzSWqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  classes = {2:'obtuse triangle', 1:'acute triangle', 0:'right triangle'}\n",
        "  inputs = torch.FloatTensor(\n",
        "      [\n",
        "          [3.0,4.0,5.0],\n",
        "       [3, 3, 3]\n",
        "       \n",
        "      ]\n",
        "  ).to(device)"
      ],
      "metadata": {
        "id": "Kesj4DYFSX9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(inputs)"
      ],
      "metadata": {
        "id": "eRbR9S0CSYm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "print(torch.round(F.softmax(outputs, dim=1), decimals=2))\n",
        "print(outputs.argmax(1))\n",
        "print(list(map(classes.get, outputs.argmax(1).tolist())))"
      ],
      "metadata": {
        "id": "O8Phno40SZLT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}